{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a91782a",
   "metadata": {},
   "source": [
    "# E.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba3191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set current path to the parent, to enable absolute imports \n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "curr_path = Path(os.getcwd()).parent\n",
    "os.chdir(curr_path)\n",
    "\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = r'C:\\Users\\Keil\\certs\\ca-bundle.crt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5666636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Keil\\anaconda3\\envs\\ai_thesis\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "import random\n",
    "import requests\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "import math\n",
    "from torch.utils.data import SequentialSampler\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "from main.active_learning.utils import seed_pool_split, experiment_AL\n",
    "from main.active_learning.datasets import ALDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878d2ab2",
   "metadata": {},
   "source": [
    "# E.2 Data Loading and Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190fa6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# put device onto GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "  print(f\"GPU name: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e545f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in: first 200 labeled instances\n",
    "# toDO: read in more data\n",
    "\n",
    "sent_df = pd.read_csv('files/datasets/labeled/l01_reuters_sample200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63788bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in: next 800 labeled instances and join\n",
    "# comment this out if only wanting to run the first iteration of the experiment\n",
    "\n",
    "send_df2 = pd.read_csv('files/datasets/labeled/l02_reuters_sample800.csv')\n",
    "\n",
    "sent_df = pd.concat([sent_df, send_df2], axis='rows',ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c29e6273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the miscellaneous instances\n",
    "\n",
    "sent_df = sent_df[sent_df['is_miscellaneous'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c153ec9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 968 instances\n"
     ]
    }
   ],
   "source": [
    "print(f'Total: {len(sent_df)} instances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc38269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sentences and valence/arousal labels as numpy arrays\n",
    "\n",
    "sentences = sent_df.sentence.values\n",
    "v_labels = sent_df.valence.values\n",
    "a_labels = sent_df.arousal.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68c0e2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "ename": "SSLError",
     "evalue": "HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/vocab.txt (Caused by SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\ai_thesis\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[0mtimeout_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m             \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpool_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai_thesis\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_get_conn\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconn\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai_thesis\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1011\u001b[0m             raise SSLError(\n\u001b[1;32m-> 1012\u001b[1;33m                 \u001b[1;34m\"Can't connect to HTTPS URL because the SSL module is not available.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m             )\n",
      "\u001b[1;31mSSLError\u001b[0m: Can't connect to HTTPS URL because the SSL module is not available.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\ai_thesis\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    498\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai_thesis\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    787\u001b[0m             retries = retries.increment(\n\u001b[1;32m--> 788\u001b[1;33m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    789\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai_thesis\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/vocab.txt (Caused by SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11296\\3116160248.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Load the BERT tokenizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loading BERT tokenizer...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert-base-uncased'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\ai_thesis\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1750\u001b[0m                         \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m                         \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1752\u001b[1;33m                         \u001b[0muser_agent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1753\u001b[0m                     )\n\u001b[0;32m   1754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai_thesis\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m             \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m         )\n\u001b[0;32m    294\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai_thesis\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m             \u001b[0m_raise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[0metag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X-Linked-Etag\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ETag\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai_thesis\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mhead\u001b[1;34m(url, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"allow_redirects\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"head\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai_thesis\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai_thesis\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    585\u001b[0m         }\n\u001b[0;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai_thesis\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ai_thesis\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    561\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_SSLError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                 \u001b[1;31m# This branch is for urllib3 v1.22 and later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 563\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSSLError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/vocab.txt (Caused by SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\"))"
     ]
    }
   ],
   "source": [
    "# toDO: check what else we can do with tokenization\n",
    "# e.g. add financial words to the vocabulary\n",
    "# e.g. take another pre-trained tokenizer model\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6f461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# measure the maximum sentence length\n",
    "# this is needed for adjusting the BERT size later\n",
    "\n",
    "\n",
    "max_len = 0\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "print('Max sentence length: ', max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        sent,  # Sentence to encode.\n",
    "        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "        max_length=70,  # Pad & truncate all sentences.\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,  # Construct attn. masks.\n",
    "        return_tensors='pt',  # Return pytorch tensors.\n",
    "    )\n",
    "\n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "v_labels = torch.tensor(v_labels)\n",
    "a_labels = torch.tensor(a_labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "print('Valence: ', v_labels[0])\n",
    "print('Arousal: ', a_labels[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21c68d6",
   "metadata": {},
   "source": [
    "# E.3 Main Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a856024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all the random seeds\n",
    "\n",
    "RANDOM_STATE = 42 \n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed_all(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b74d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment parameters\n",
    "\n",
    "TRAIN_SIZE = 0.75\n",
    "methods = ['random', 'farthest-first', 'mc-dropout']\n",
    "batch_sizes = [16, 32, 64]  # 4,\n",
    "lrs = [1e-5, 1e-6, 1e-7]  # 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate results storage for valence and arousal models\n",
    "\n",
    "v_results, a_results = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db7669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all batch_sizes\n",
    "for batch_size in batch_sizes:\n",
    "    \n",
    "    # initiate storage in dictionary, in a nested manner\n",
    "    v_results[batch_size], a_results[batch_size] = {}, {}\n",
    "    \n",
    "    # loop over all the learning rates \n",
    "    for lr in lrs:\n",
    "        \n",
    "        # initiate storage in dictionary, in a nested manner\n",
    "        v_results[batch_size][lr], a_results[batch_size][lr] = {}, {}\n",
    "        \n",
    "        # loop over all the methods\n",
    "        for method in methods:\n",
    "            \n",
    "            # initiate storage in dictionary, in a nested manner\n",
    "            v_results[batch_size][lr][method], a_results[batch_size][lr][method] = {}, {}\n",
    "        \n",
    "            # print out modifications of this experiment loop\n",
    "            print(\"--\" * 20)\n",
    "            print(f\"METHOD: {method}. BATCH SIZE: {batch_size}. LEARNING RATE: {lr} \")\n",
    "            print(\"--\" * 20)\n",
    "\n",
    "            # seperate 25% for testing, 75% training\n",
    "            v_train_ds, v_test_ds = seed_pool_split(input_ids, attention_masks, v_labels, seed_size=TRAIN_SIZE,\n",
    "                                                    random_state=RANDOM_STATE)\n",
    "            a_train_ds, a_test_ds = seed_pool_split(input_ids, attention_masks, a_labels, seed_size=TRAIN_SIZE,\n",
    "                                                    random_state=RANDOM_STATE)\n",
    "\n",
    "            # initiate seed and pool\n",
    "            v_seed, v_pool = seed_pool_split(v_train_ds[0], v_train_ds[1], v_train_ds[2], seed_size=batch_size,\n",
    "                                             random_state=RANDOM_STATE)\n",
    "            a_seed, a_pool = seed_pool_split(a_train_ds[0], a_train_ds[1], a_train_ds[2], seed_size=batch_size,\n",
    "                                             random_state=RANDOM_STATE)\n",
    "\n",
    "            #v_results[method], a_results[method] = {}, {}\n",
    "            #v_results[method][batch_size], a_results[method][batch_size] = {}, {}\n",
    "\n",
    "            # download valence and arousal base models\n",
    "            print(\"Downloading Valence model...\")\n",
    "            torch.manual_seed(42)\n",
    "            v_model = BertForSequenceClassification.from_pretrained(\n",
    "                'bert-base-uncased',\n",
    "                num_labels=1,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=True)\n",
    "\n",
    "            print(\"Downloading Arousal model...\")\n",
    "            torch.manual_seed(42)\n",
    "            a_model = BertForSequenceClassification.from_pretrained(\n",
    "                'bert-base-uncased',\n",
    "                num_labels=1,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=True)\n",
    "\n",
    "            # initialize Active Learning datasets\n",
    "            v_seed_ds, v_pool_ds = ALDataset(v_seed[0], v_seed[1], v_seed[2]), ALDataset(v_pool[0], v_pool[1],\n",
    "                                                                                         v_pool[2])\n",
    "\n",
    "            a_seed_ds, a_pool_ds = ALDataset(a_seed[0], a_seed[1], a_seed[2]), ALDataset(a_pool[0], a_pool[1],\n",
    "                                                                                         a_pool[2])\n",
    "\n",
    "            # take a subsample only\n",
    "#             SAMPLE_SIZE = 12\n",
    "#             RANDOM_SEED = 42\n",
    "#             v_pool_ds = v_pool_ds.subsample(SAMPLE_SIZE, RANDOM_SEED)\n",
    "#             a_pool_ds = a_pool_ds.subsample(SAMPLE_SIZE, RANDOM_SEED)\n",
    "\n",
    "#             print(\"Valence Sample Pool Size: \", len(v_pool_ds), \"Arousal Sample Pool Size: \", len(a_pool_ds))\n",
    "\n",
    "            # initate testsets\n",
    "            v_test_set = TensorDataset(v_test_ds[0], v_test_ds[1], v_test_ds[2])\n",
    "            a_test_set = TensorDataset(a_test_ds[0], a_test_ds[1], a_test_ds[2])\n",
    "\n",
    "            # start the experiments\n",
    "            print(\"--\" * 20)\n",
    "            print(\"RUNNING VALENCE EXPERIMENT\")\n",
    "            print(\"--\" * 20)\n",
    "            v_train_rmse_curve, v_test_loss_curve = experiment_AL(v_seed_ds, v_pool_ds, v_test_set, v_model, method, lr,\n",
    "                                                                  batch_size, device)\n",
    "\n",
    "            print(\"--\" * 20)\n",
    "            print(\"RUNNING AROUSAL EXPERIMENT\")\n",
    "            print(\"--\" * 20)\n",
    "            a_train_rmse_curve, a_test_loss_curve = experiment_AL(a_seed_ds, a_pool_ds, a_test_set, a_model, method, lr,\n",
    "                                                                  batch_size, device)\n",
    "\n",
    "            # store results\n",
    "            v_results[batch_size][lr][method]['train'], v_results[batch_size][lr][method]['test'] = v_train_rmse_curve, v_test_loss_curve\n",
    "            a_results[batch_size][lr][method]['train'], a_results[batch_size][lr][method]['test'] = a_train_rmse_curve, a_test_loss_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415aaa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce graph\n",
    "\n",
    "\n",
    "def plot_al_results(v_results, a_results, batch_size, lr):\n",
    "\n",
    "    fig = plt.figure(figsize=(16,10))\n",
    "\n",
    "    gs = fig.add_gridspec(nrows=2, ncols=2, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    axs = gs.subplots(sharex=True)\n",
    "\n",
    "\n",
    "    for i, method in enumerate(v_results[batch_size][lr].keys()):\n",
    "        axs[0, 0].plot(v_results[batch_size][lr][method]['train'], label=method)\n",
    "        axs[1, 0].plot(v_results[batch_size][lr][method]['test'], label=method)\n",
    "        axs[0, 0].legend(); axs[1, 0].legend()\n",
    "        axs[0, 0].grid(); axs[1, 0].grid()\n",
    "\n",
    "        axs[0,0].set_title('Valence')\n",
    "        axs[0,0].set_ylabel('Training')\n",
    "        axs[1,0].set_ylabel('Testing')\n",
    "\n",
    "    for i, method in enumerate(a_results[batch_size][lr].keys()):\n",
    "        axs[0, 1].plot(a_results[batch_size][lr][method]['train'], label=method)\n",
    "        axs[1, 1].plot(a_results[batch_size][lr][method]['test'], label=method)\n",
    "        axs[0, 1].legend(); axs[1, 1].legend()\n",
    "        axs[0, 1].yaxis.tick_right(); axs[1, 1].yaxis.tick_right()\n",
    "        axs[0, 1].grid(); axs[1, 1].grid()\n",
    "\n",
    "        axs[0, 1].set_title('Arousal')\n",
    "\n",
    "    fig.text(0.5, 0.07, 'Number of Epochs', ha='center')\n",
    "    fig.text(0.05, 0.5, 'RMSE', ha='center', va='center', rotation='vertical')\n",
    "\n",
    "    plt.savefig(f'files/results/active_learning_experiments/al_exp_batch_size_{batch_size}_learning_rate_{lr}_full.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5263f28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size in batch_sizes:\n",
    "    for lr in lrs:\n",
    "        plot_al_results(v_results, a_results, batch_size, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6349f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20740182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to structure the results\n",
    "\n",
    "v_res_summ = {}\n",
    "\n",
    "v_res_summ['random'] = []\n",
    "v_res_summ['farthest-first'] = []\n",
    "v_res_summ['mc-dropout'] = []\n",
    "\n",
    "for key in v_results.keys():\n",
    "    for subkey in v_results[key].keys():\n",
    "        for method in v_results[key][subkey].keys():\n",
    "            v_res_summ[method].append(np.mean(v_results[key][subkey][method]['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f802796",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_res_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0daf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a table to summarize the results\n",
    "\n",
    "idx_arrs = [\n",
    "    [16, 16, 16, 32, 32, 32, 64, 64, 64],\n",
    "    [1e-5, 1e-6, 1e-7, 1e-5, 1e-6, 1e-7, 1e-5, 1e-6, 1e-7]\n",
    "    ]\n",
    "\n",
    "idx_tuples = list(zip(*idx_arrs))\n",
    "\n",
    "index = pd.MultiIndex.from_tuples(idx_tuples, names=[\"batch_size\", \"learning_rate\"])\n",
    "\n",
    "# construct the results dataframe\n",
    "\n",
    "v_res_summ_df = pd.DataFrame(v_res_summ, index=index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48300308",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v_res_summ_df.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
